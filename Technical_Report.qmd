---
title: "DATA 624 Project 2: Technical Report"
format: html
author: "Ali Ahmed, Andreina Arias, Kaylie Evans, Naomi Buell, and Zaneta Paulusova"
date: "`r Sys.Date()`"
---

## Instructions

*This is role playing. I am your new boss. I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me. My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.*

*Please use the historical data set I am providing. Build and report the factors in BOTH a technical and non-technical report. I like to use Word and Excel. Please provide your non-technical report in a business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach.*

*Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports. Also submit the excel file showing the prediction of your models for pH.*

## Introduction

In this technical report, we will analyze the manufacturing process of ABC Beverage to understand the factors that influence the pH level of the product. We will build predictive models to forecast pH levels based on various predictors in the dataset. The goal is to provide insights into the manufacturing process and to create a model that can be used for future predictions.

## Setup

We will use the following packages for data analysis, modeling, and visualization.

```{r}
#| label: load packages

Sys.setenv(JAVA_HOME = "C:/Program Files/TreeAgePro/jre18162")

library(DataExplorer)
library(tidyverse)
library(readxl)
library(fpp3)
library(caret)
library(skimr)
library(doParallel)
library(RWeka)
library(rpart)
library(corrplot)
library(openxlsx)
library(tidyr)
library(mice)
library(janitor)
library(dplyr)
```

We will also set the seed for reproducibility.

```{r}
#| label: set-seed

set.seed(1234)
```

Since these models take a long time to train, we'll utilize the `doParallel` package for speed.

```{r}
#| label: parallel-processing
#| warning: FALSE
#| message: FALSE

cluster <- makeCluster(
    detectCores() - 1
)

registerDoParallel(
    cluster
)
```

## Load data

First, we load the data.

```{r}
#| label: load-data

StudentData <- read_excel("StudentData.xlsx") # Training data
StudentEvaluation <- read_excel("StudentEvaluation.xlsx") # Test data
```

## Tidy Data and Transformation

In this section, we mutate variable types (e.g., convert character column `Brand Code` to factor, numeric, etc., so we can use this to forecast data) and impute missing data where needed. We also check correlation, removing features over a threshold to avoid multicollinearity, and remove near zero variance predictors with `nearZeroVar()`.

```{r}
#| echo: false
str(StudentData) # Column names are with spaces will clean up to make it easier to work with in R.
```

We impute missing data below. It is best to impute data when missing data is it less than 5%, brand code rounds up to 5% which is why the missing data for brand code will be removed rather.

```{r}
#| echo: false

Missing <- (colSums(is.na(StudentData)) / 2571) * 100
print(Missing)
```

The missing data for brand code were removed as imputation wouldn't appripiate and leaving it missing would cause misleading results in our analysis. For the missing data the MICE function was used to impute missing data that are numerical.

```{r}
#| include: false

StudentData <- StudentData |>
    drop_na("Brand Code")
```

```{r}
#| include: false
StudentData <- mice(
    StudentData,
    method = "pmm", # pmm=predictive mean matching
    m = 5, # number of imputed dataset usually 5 is default, keep in mind it will take some time.
    maxit = 5, # number of max iteration
    seed = 10
) |>
    complete()

```

Removed near zero variance variables.

```{r}
#| include: false

# Save removed column name
nzv_cols <- names(StudentData)[nearZeroVar(StudentData)]
# Remove near zero variance columns
StudentData <- StudentData[, -nearZeroVar(StudentData)]
```

`r nzv_cols` was removed as it had near zero variance.

```{r}
#| echo: false
# Check for missing data again
Missing <- colSums(is.na(StudentData))
print(Missing) # No missing data and hyd_pressure1 was removed as it was the variable with a variance near zero

```

Now there is no missing data and `hyd_pressure1` was removed as it was the variable with a variance near zero.

```{r, Correlation}

cor_matrix <- StudentData |>
    select_if(is.numeric) |>
    cor()

# Set threshold for correlation
threshold <- 0.9

# Removed columns with correlation over threshold
high_cor_indices <- findCorrelation(cor_matrix, cutoff = threshold)

# Removed those columns from StudentData
StudentData <- StudentData[, -high_cor_indices]
# Removed hyd_pressure2, hyd_presure4, filler_level, carb_flow , mfr, air_pressure

cor_matrix2 <- StudentData |>
    select_if(is.numeric) |>
    cor()
```

```{r}
## Correlation Plot
corrplot(
    cor_matrix2,
    method = "color",
    type = "upper",
    tl.col = "black",
    tl.cex = .8,
    addCoef.col = "black",
    number.cex = .5,
    diag = FALSE
)
```

```{r}
# missing data in StudentEvaluation
Missing2 <- colSums(is.na(StudentEvaluation))
print(Missing2)
```

```{r}
StudentEvaluation <- mice(
    StudentEvaluation,
    method = "pmm",
    m = 5,
    maxit = 5,
    seed = 10
) |>
    complete()

```

The imputation left `pH` and `brand_code` columns untouched.

Lastly, we change all variable types to numeric.

```{r}
StudentData <- StudentData[, sapply(
    StudentData,
    is.numeric
)]
```

## Exploratory Data Analysis

We browse the data below.

```{r}
#| label: browse-data

StudentData |> skim()
StudentEvaluation |> skim()
```

Below, we will perform exploratory data analysis (EDA) to understand the dataset better. We will summarize the data, visualize distributions, and check for missing values. We will also explore relationships between predictors and the response variable (pH).

```{r}
#| label: eda

# Summary statistics
summary(StudentData)

# Histograms
plot_histogram(StudentData)

# Summarize missing data
StudentData |>
    summarise(across(everything(), ~ sum(is.na(.)))) |>
    pivot_longer(everything()) |>
    filter(value > 0) |>
    arrange(desc(value))

# Plot missing data
plot_missing(StudentData)

# Correlation matrix
correlation_matrix <- StudentData |>
    select(where(is.numeric)) |>
    cor(use = "pairwise.complete.obs")
corrplot(
    correlation_matrix,
    order = "hclust",
    # addCoef.col = "grey",
    # addCoefasPercent = TRUE,
    tl.cex = 0.7
)

# Feature plot
featurePlot(
    x = StudentData |>
        select(where(is.numeric)) |>
        select(-PH),
    y = StudentData$PH,
    plot = "scatter"
)
```

From the visualizations and summary statistics above, we can observe:

1.  Distribution patterns:
    -   Some variables show some skewness, e.g., `Filler.Speed`, `MFR`, `Oxygen.Filler`, and `Usage.cont`
    -   Many variables are bimodal, e.g., `Air.Pressurer`, `Balling`, `Balling.Lvl`, `Carb.Flow`, `Carb.Rel`, and `Density`
    -   The target variable `PH` has a relatively normal distribution
2.  Correlations:
    -   Several predictors show moderate correlations with `PH`, e.g., `Mnf Flow`, `Usage cont`, `Filler Level`, `Bowl Setpoint`, and `Pressure Setpoint`
    -   There are some strong correlations between predictors, suggesting potential multicollinearity. We should address multicollinearity in the data cleaning phase
3.  Missing values:
    -   Several variables contain missing values that will need to be addressed, especially `MFR`

## Forecast

We will try the following models:

-   Linear regression models:
    -   PLS
    -   Elastic Net
-   Non-linear regression models:
    -   KNN
    -   Multivariate Adaptive Regression Splines (MARS)
-   Tree-based regression models:
    -   Single Tree
    -   Model Tree

Since the data is relatively large, we will avoid heavy models like neural networks, support vector machines (SVM), or boosted trees. Since we have a lot of predictors, we opt for models that handle feature selection or dimensionality reduction like elastic net, PLS, and tree-based models. We also want model results to be easy to interpret for our non-technical report, so we focus on elastic net, MARS, and single/model trees.

Also, because the data is relatively large, we can afford to use a training and a test set. We also center and scale the data to ensure that all predictors are on the same scale, which is important for models like PLS and elastic net.

```{r}
#| label: split-data

# <!--NB to DELETE this line after imputation section is completed-->
StudentData <- StudentData |> drop_na()

# Split the data into a training and a test set
trainingRows <- createDataPartition(
    StudentData$PH,
    p = .80,
    list = FALSE
)
StudentData_train <- StudentData[trainingRows, ]
StudentData_test <- StudentData[-trainingRows, ]

# Save training and test predictors and response variables
StudentData_train_x <- StudentData_train[, !names(StudentData_train) %in% "PH"]
StudentData_train_y <- StudentData_train$PH
StudentData_test_x <- StudentData_test[, !names(StudentData_test) %in% "PH"]
StudentData_test_y <- StudentData_test$PH
```

Fitting models to the training data below.

```{r}
#| label: pls

# Pre-process the data and tune a PLS model
ctrl <- trainControl(method = "cv", number = 10)

# Train
plsTune <- train(
    StudentData_train_x,
    StudentData_train_y,
    method = "pls",
    tuneLength = 20,
    trControl = ctrl,
    preProc = c("center", "scale")
)
plsTune

# Predict
plsPred <- predict(
    plsTune,
    newdata = StudentData_test_x
)
```

```{r}
#| label: elastic-net
#| warning: false

# Train
enetGrid <- expand.grid(
    .lambda = c(0, 0.01, .1),
    .fraction = seq(.05, 1, length = 20)
)

enetTune <- train(
    StudentData_train_x,
    StudentData_train_y,
    method = "enet",
    tuneGrid = enetGrid,
    trControl = ctrl,
    preProc = c("center", "scale")
)
enetTune

# Predict
enetPred <- predict(
    enetTune,
    newdata = StudentData_test_x
)
```

```{r}
#| label: knn
#| warning: false

# Train
knnTune <- train(
    StudentData_train_x,
    StudentData_train_y,
    method = "knn",
    preProc = c("center", "scale"),
    tuneLength = 10
)
knnTune

# Predict
knnPred <- predict(
    knnTune,
    newdata = StudentData_test_x
)
```

```{r}
#| label: mars
#| warning: false

# Train
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)

marsTune <- train(
    StudentData_train_x,
    StudentData_train_y,
    method = "earth",
    tuneGrid = marsGrid,
    trControl = trainControl(method = "cv")
)
marsTune

marsPred <- predict(
    marsTune,
    newdata = StudentData_test_x
)
```

```{r}
#| label: single-tree
#| warning: false

# Train
rpartTune <- train(
    StudentData_train_x,
    StudentData_train_y,
    method = "rpart2",
    tuneLength = 10,
    trControl = trainControl(method = "cv")
)
rpartTune

# Predict
rpartPred <- predict(
    rpartTune,
    newdata = StudentData_test_x
)
```

```{r}
#| label: model-tree
#| warning: false

# Train
m5Tune <- train(
    StudentData_train_x,
    StudentData_train_y,
    method = "M5",
    trControl = trainControl(method = "cv"),
    control = Weka_control(M = 10)
)
m5Tune

# Predict
m5Pred <- predict(
    m5Tune,
    newdata = StudentData_test_x
)
```

## Compare model fit and select optimal model

Next, we compare fit and pick model to export.

```{r}
#| label: nonlinear-regression-models

ranking <- data.frame(
    Model = c(
        "PLS",
        "Elastic Net",
        "KNN",
        "MARS",
        "Single Tree",
        "Model Tree"
    ),
    rbind(
        postResample(pred = plsPred, obs = StudentData_test_y),
        postResample(pred = enetPred, obs = StudentData_test_y),
        postResample(pred = knnPred, obs = StudentData_test_y),
        postResample(pred = marsPred, obs = StudentData_test_y),
        postResample(pred = rpartPred, obs = StudentData_test_y),
        postResample(pred = m5Pred, obs = StudentData_test_y)
    )
) |>
    arrange(RMSE)
ranking
best <- ranking[1, 1]
```

The **`r best` model** gives the optimal resampling and test set performance since it has the lowest RMSE and highest R squared. 

We plot the importance of the variables used, as well as the predicted vs actual values below.

```{r}
#| label: plot-model

# Plot variable importance for MARS model
plot(varImp(marsTune))

# Plot predicted vs actual values
pred_data <- data.frame(
    predicted = as.numeric(marsPred),
    actual = as.numeric(StudentData_test_y)
)
ggplot(pred_data, aes(x = actual, y = predicted)) +
    geom_point(alpha = 0.5) +
    geom_abline(color = "red") +
    labs(
        x = "Actual pH",
        y = "Predicted pH",
    ) +
    coord_fixed(ratio = 1) +
    theme_classic() +
    scale_x_continuous(breaks = seq(8, 10, by = 0.2)) +
    scale_y_continuous(breaks = seq(8, 10, by = 0.2))
```

The trend line is close to the 45-degree line, indicating that the model is performing well. The points are scattered around the line, suggesting that the model is able to predict pH levels accurately.

## Predict pH for test set

```{r}
#| label: predict-ph

# Predict pH for the test set using the best model
if (best == "PLS") {
    finalTune <- plsTune
} else if (best == "Elastic Net") {
    finalTune <- enetTune
} else if (best == "KNN") {
    finalTune <- knnTune
} else if (best == "MARS") {
    finalTune <- marsTune
} else if (best == "Single Tree") {
    finalTune <- rpartTune
} else if (best == "Model Tree") {
    finalTune <- m5Tune
}

StudentEvaluation_x <- StudentEvaluation |>
    select(-PH)

# Combine predictions with the test set
predictions <- data.frame(
    StudentEvaluation_x,
    PH = predict(finalTune, newdata = StudentEvaluation_x)
) |>
    rename(PH = y)
```

## Export

```{r}
#| label: Export to Excel

# Export predictions to Excel
write.xlsx(predictions, "PH_forecasts.xlsx")
```

## Conclusion

**The forecasts for PH are saved as "PH_forecasts.xlsx", using the `r best` model.**
